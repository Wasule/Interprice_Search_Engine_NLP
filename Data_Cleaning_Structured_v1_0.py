#Run time calculations
#import time
#start=time.process_time()

import pandas as pd
import pymysql

import re

import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import stopwords

def get_data_from_sql(database,table, password='w12345', host='localhost', port=3306, user='root'):
    """
    This function is used for connecting and extracting a table from an sql server.
    
    The arguments are:
        database*: Name of the database from which the table has to be extracted.\n
        table*: Name of the table to be extracted.\n
        password*: Password to connect with the server.\n
        host: Host name or id (default is 'localhost')\n
        port: Port value (default is 3306)\n
        user: User name (dafault is 'root')
    
    Output is a pandas.DataFrame type object.
    """
    try:
        connect=pymysql.connect(host=host,port=port,user=user,password=password,database=database)
    except pymysql.InternalError:
        print('Can not find the database \'{}\''.format(database))
    except Exception as e:
        print(e)
    else:
        try:
            df=pd.read_sql_query('select * from {}'.format(table),connect)
            return df
        except Exception as e:
            print(e)

def null_value_treatment(df):
    """
    If all elements of a document(row) are null then, we drop that row as it plays no importance in searching since there is nothing to search.\n
    If some (but not all) elements of a document(row) are null then, we make it empty since the row can still be searched but with limited information.\n\n
    Input and output both are pandas.DataFrame type object.    
    """
    df=df.dropna(how='all')
    df=df.fillna("")
    return df

def remove_extreme_ends_spaces(df):
    """
    This function removes the leading and trailing end spaces of all object datatype columns.\n\n
    Input and output both are pandas.DataFrame type object.
    """
    return df.apply(lambda x: x.astype(str).str.strip() if x.dtype=='object' else x)

def concatenate_fields(df):
    """
    It concatenates all the columns to a single column.\n
    Input is pandas.DataFrame type object.\n
    Output is pandas.Series type object.
    """
    return df[df.columns].apply(lambda x: ' '.join(x.values.astype(str)),axis=1)
    

def clean_medical_text(text):
    """
    This function cleans or parses the medical data to a much more manageble form which is required to train the search engine.\n
    Input and output both are string.
    """
    text=text.lower()
    pattern=re.compile(r'((\d+)(\.?)(\d*))(\s)') #Detects Dosages followed by a blank space
    text=re.sub(pattern,r"\1",text) #Removes that blank space which will help for better tokenization later
    text=re.sub(r'[+,:;-_`()]'," ",text) #Removes some punctuations present in the data
    text=re.sub(r'(/)(\d|ml|mg)',r'\2',text) #Removing forward slashes expect when it comes to concentration of a drug
    text=re.sub(r'[\s]+'," ",text) #Replaces excess white spaces with only 1 black space
    return text
        
def clean_products_text(text):
    """
    This function cleans or parses the products data to a much more manageble form which is required to train the search engine.\n
    Input and output both are string.
    """
    text=text.lower() #All text in lower case
    text=re.sub(r'[(\W)(_)]+',' ',text) #Remove all punctuations including underscore
    text=re.sub(r'[\s]+',' ',text) #Remove excess white spaces if any
    return text

def tokenizing_data_tolist(df):
    """
    This function converts the data into token of words in a list.\n
    Input is a pandas.Series type object.\n
    Output is a list type object.
    """
    token_list=[]
    for i in range(len(df)):
        token_text=word_tokenize(df[i])
        token_list.append(token_text)
    return token_list

def pos_tagger(doclist):
    """
    This function takes a list (in which every row represents a document) and determines the parts of speech associated with every word.\n
    This function uses nltk.pos_tag to tag the words.\n
    Input and output both are a list.
    """
    for doc in doclist:
        try:
            tagged.append(pos_tag(doc))
        except:
            tagged=[]
            tagged.append(pos_tag(doc))
    return tagged

def get_wordnet_pos(doclist):
    """
    This functions maps the pos generated by nltk.pos_tag to the pos by wordnet.\n
    WordNetLemmatizer.lemmatize() function uses the pos type of wordnet (not the pos_tag ones).\n
    The input is a list of all the pos generated by nltk.pos_tag function for all the documents(row).\n
    The output is a mapped list of the pos accepted by wordnet for all the documents(rows).
    """
    tag_dict={"J": wordnet.ADJ,
              "N": wordnet.NOUN,
              "R": wordnet.ADV,
              "V": wordnet.VERB}
    new_doclist=[]
    for doc in doclist:
        new_doc=[]
        for i in range(len(doc)):
            tag=doc[i][1][0].upper()
            pos=list(doc[i])
            pos.append(tag_dict.get(tag,wordnet.NOUN))
            new_doc.append(pos)
        new_doclist.append(new_doc)
    return new_doclist

def lemmatized_list(pos_doclist):
    """
    This function lemmatizes the words using WordNetLemmatizer().lemmatize() funtion.\n
    The input is a list of all documents in which each document has a sublist (of size 3) in the form of a [word, pos from pos_tag() function, wordnet pos].\n
    Or The input is the return object of get_wordnet_pos() function.\n
    The output is a list of all documents with all words lemmatized to their root forms using the wordnet pos supplied.
    """
    lemma=WordNetLemmatizer()
    lem_word=[]
    for doc in pos_doclist:
        lem_word.append([lemma.lemmatize(w[0],w[2]) for w in doc])
    return lem_word

def stop_words_removal(doclist):
    """
    This function removes the stopwords (english language) from all the documents and also drops the words with length of 1 or 0.\n
    The input is a list of all the documents.\n
    The output is also a list of all the documents without any stopwords.
    """
    stop_words=stopwords.words("english")
    for i in range(len(doclist)):
        doclist[i]=[w for w in doclist[i] if w not in stop_words and len(w)>1]
    return doclist


###Main 

# Provide default empty datasets so importing this module is safe.
med_data = pd.DataFrame()
products_data = pd.DataFrame()


def build_datasets_from_sql():
    # This encapsulates the original top-level dataset construction so it
    # doesn't run on import. Call directly for local development.
    # For safety this function will attempt to load from SQL; callers
    # can catch exceptions or skip it in production.
    #For medical data
    med_data_local = get_data_from_sql(database='classicmodels',password='Gaurav@12345',table='med_data')
    med_data_local=null_value_treatment(med_data_local)
    med_data_clean=med_data_local.replace(to_replace='Ear &Mouth',value='Ear & Mouth')
    med_data_clean=remove_extreme_ends_spaces(med_data_clean)

    med_data_combined=med_data_clean.drop(['ID1','ID2','PackSize','MRP'],axis=1)
    med_data_combined=concatenate_fields(med_data_combined)
    med_data_combined=med_data_combined.apply(lambda x: clean_medical_text(x))

    all_med_data=tokenizing_data_tolist(med_data_combined)

    med_data_short=all_med_data[0:1000] ###Note: Shortened to save time. Must be removed in future.
    med_data_pos=pos_tagger(med_data_short)
    med_data_pos=get_wordnet_pos(med_data_pos)

    all_med_data=lemmatized_list(med_data_pos)
    all_med_data=stop_words_removal(all_med_data)

    #For products data
    products_data_local=get_data_from_sql(database='classicmodels',password='w12345',table='products')
    products_data_local=null_value_treatment(products_data_local)
    products_data_clean=remove_extreme_ends_spaces(products_data_local)

    products_data_combined=products_data_clean.drop(['quantityInStock','buyPrice','MSRP'],axis=1)
    products_data_combined=concatenate_fields(products_data_combined)
    products_data_combined=products_data_combined.apply(lambda x: clean_products_text(x))

    all_products_data=tokenizing_data_tolist(products_data_combined)

    products_data_pos=pos_tagger(all_products_data)
    products_data_pos=get_wordnet_pos(products_data_pos)

    all_products_data=lemmatized_list(products_data_pos)
    all_products_data=stop_words_removal(all_products_data)

    return med_data_clean, products_data_clean


if __name__ == "__main__":
    # When run directly, build datasets (useful for development)
    try:
        med_data, products_data = build_datasets_from_sql()
    except Exception as e:
        print("Failed to build datasets from SQL:", e)

#Run Time calculations
#stop=time.process_time()

#print(start)
#print(stop)
#print(stop-start) 
#It took 396 seconds (6 minutes 36 secs) to run the entire file (with full dataset).


###To Do:

#In line 186, we have only taken 1000 documents just for now to decrease time and to test and debug it for errors.
#We must have to consider all the documents (instead of just 1000) at some point in future.

### Nomenclature:

#list name all_med_data and all_products_data need will be used to create the inverted index.
#list name med_data_clean and products_data_clean will be used to display to the user in the front end.
